# KeyPilot Model Configuration

model:
  name: "keypilot_vlm"
  vocab_size: 119547  # mBERT multilingual tokenizer vocab size
  d_model: 256
  num_tasks: 3  # error_correction, auto_completion, suggestion
  num_layouts: 5  # EN, ZH, SYM, EMOJI, NUM
  num_experts: 5  # Language experts
  pretrained_backbone: true
  user_emb_dim: 64

encoder:
  backbone:
    type: "mobilevit_xxs"
    width_multiplier: 0.75
    pretrained: true
    out_channels: 192
  
  sam_lite:
    in_channels: 192
    num_masks: 4  # input_field, chat_bubble, keyboard, title_bar
  
  global_projection:
    in_channels: 192
    out_dim: 256
  
  text_encoder:
    type: "mbert_tiny"
    vocab_size: 119547  # mBERT multilingual
    hidden_size: 312
    num_layers: 2
    num_heads: 12
    intermediate_size: 1248
    max_position_embeddings: 64
    out_dim: 256
  
  cross_former:
    d_model: 256
    num_heads: 8
    ffn_dim: 1024
    num_layers: 1

decoder:
  task_router:
    d_model: 256
    num_tasks: 3
    hidden_dim: 128
    threshold: 0.7
  
  layout_router:
    d_model: 256
    num_layouts: 5
    num_layers: 2
    num_heads: 8
    ffn_dim: 1024
    alpha: 0.3  # Temporal bias
    threshold: 0.8  # Stability threshold
  
  language_moe:
    d_model: 256
    num_experts: 5
    hidden_dim: 128
    top_k: 2
    threshold: 0.7
  
  expert:
    d_model: 256
    num_heads: 8
    ffn_dim: 1024
    num_layers: 1

vocabulary:
  type: "multilingual_bpe"
  size: 119547  # mBERT vocab size
  special_tokens:
    - "[PAD]"
    - "[UNK]"
    - "[CLS]"
    - "[SEP]"
    - "[MASK]"
    - "<ERR>"
    - "<COMP>"
    - "<SUG>"
    - "<EN>"
    - "<ZH>"
    - "<SYM>"
    - "<EMOJI>"
    - "<NUM>"

training:
  # Loss weights
  loss:
    lambda_enc: 0.3
    lambda_dec: 1.0
    lambda_align: 1.0
    lambda_contrastive: 0.2
    lambda_task: 0.5
    lambda_layout: 0.4
    lambda_consistency: 0.3
    lambda_load: 0.01
    temperature: 0.07
  
  # Optimizer
  optimizer:
    type: "adamw"
    lr: 3.0e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8
  
  # Scheduler
  scheduler:
    type: "cosine"
    warmup_steps: 1000
    total_steps: 100000
    min_lr: 1.0e-6
  
  # Training settings
  batch_size: 32
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  mixed_precision: "fp16"
  num_epochs: 10
  
  # Checkpointing
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  save_total_limit: 5

data:
  # Image preprocessing
  image:
    height: 512
    width: 256
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  
  # Text preprocessing
  text:
    max_length: 64
    truncation: true
    padding: "max_length"
  
  # Data paths
  train_data: "data/train"
  val_data: "data/val"
  test_data: "data/test"

inference:
  # Generation settings
  max_length: 5
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  num_beams: 1
  
  # Latency targets
  target_latency_ms: 50
  encoder_latency_ms: 19
  decoder_latency_ms: 10
  layout_switch_ms: 5

deployment:
  # Quantization
  quantization:
    enabled: true
    method: "int8"
    calibration_samples: 1000
  
  # Optimization
  optimization:
    onnx_export: true
    trt_conversion: false
    mobile_export: true
  
  # Resource constraints
  constraints:
    max_model_size_mb: 50
    max_ram_usage_mb: 100
    max_cpu_usage_percent: 30
    max_battery_impact_percent: 5

